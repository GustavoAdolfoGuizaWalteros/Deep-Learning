{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2_2_PLN_redes neuronales recurrentes (RNN).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "k6zNNLvrPR1f",
        "FshwyfFykB_4",
        "SKuJocQwCLg1"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GustavoAdolfoGuizaWalteros/Deep_Learning/blob/main/DeepLearning/PLN/Torah.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVozdGnPDqp3"
      },
      "source": [
        "#!kill -9 -1 #reiniciando la maquina virtual"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul9CHmeeRwy1",
        "outputId": "2dafcb52-5c37-46a5-bd81-73e83a8f7de3"
      },
      "source": [
        "#!pip list\n",
        "!nvcc --version  #Version de CUDA en la maquina virtual"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGa1tvf5t5r4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import timeit               #para medir tiempos\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import os\n",
        "import time\n",
        "import sys"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXlRSrBJrK9T",
        "outputId": "503567ce-5275-4e64-f29b-03b183c83a9d"
      },
      "source": [
        "print(\"Tensorflow Version: \", tf.__version__)\n",
        "print(\"Dispositivos disponibles para entrenar: \", tf.config.list_physical_devices())\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Encontrada la GPU: {}'.format(device_name))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow Version:  2.7.0\n",
            "Dispositivos disponibles para entrenar:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Encontrada la GPU: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8hkb1wlQKET"
      },
      "source": [
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)   "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsqDzp_4Q0nV",
        "outputId": "52dc595c-8f8a-4bc4-f78e-f9db925eb919"
      },
      "source": [
        "cpu()  #ejecutamos entrenamiento con CPU\n",
        "gpu()  #ejecutamos entrenamiento con GPU\n",
        "# Run the op several times.\n",
        "print('TIEMPO (seg) para entrenar una red convolucional de 32x7x7x3 filtros sobre un randomico de 100x100x100x3 imagenes '\n",
        "      '(batch x height x width x channel). suma de 10 epochs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TIEMPO (seg) para entrenar una red convolucional de 32x7x7x3 filtros sobre un randomico de 100x100x100x3 imagenes (batch x height x width x channel). suma de 10 epochs.\n",
            "CPU (s):\n",
            "3.6845605359999922\n",
            "GPU (s):\n",
            "0.053426725000008446\n",
            "GPU speedup over CPU: 68x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9E2ihO2rLjM",
        "outputId": "6fb3ca3d-b7a9-462e-a9df-7596e8aac519"
      },
      "source": [
        "#tf.device('/gpu:0') #activando la CPU\n",
        "tf.device('/device:GPU:0') #activando la GPU "
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.eager.context._EagerDeviceContext at 0x7f51c8465cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PETkDFl3Mff"
      },
      "source": [
        "fileDL= tf.keras.utils.get_file('Torah.txt','https://raw.githubusercontent.com/GustavoAdolfoGuizaWalteros/Deep_Learning/main/Cuentos_txt/Torah.txt')\n",
        "texto = open(fileDL, 'rb').read().decode(encoding='utf-8')\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SbnxWwxfNnH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dbe3ec7-7a01-42fe-ad43-d24724eab4ad"
      },
      "source": [
        "import re\n",
        "from unicodedata import normalize\n",
        "#pasa todo a minuscula\n",
        "texto     = texto.lower()\n",
        "#reemplazar tildes por letras similares sin tildes\n",
        "transfor  = dict.fromkeys(map(ord, u'\\u0301\\u0308'), None)\n",
        "texto     = normalize('NFKC', normalize('NFKD', texto).translate(transfor))\n",
        "#quitar saltos de linea\n",
        "texto      = texto.strip()\n",
        "texto      = re.sub('\\r|\\n', ' ',texto)\n",
        "#quitar espacios dobles\n",
        "texto      = re.sub(' +', ' ', texto)\n",
        "#quitando caracteres especiales\n",
        "texto = re.sub(r\"[^a-zA-Z0-9]+\",\" \",texto)\n",
        "print(texto)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpxHpVkcys57",
        "outputId": "b3113157-0d99-4e2a-a8be-ece90a32c4f9"
      },
      "source": [
        "print('el texto tiene longitud de:{} caracteres'. format(len(texto)))\n",
        "vocab = sorted(set(texto))\n",
        "print('el texto esta compuesto de estos :{} caracteres'. format(len(vocab)))\n",
        "print(vocab)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "el texto tiene longitud de:4906052 caracteres\n",
            "el texto esta compuesto de estos :37 caracteres\n",
            "[' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUZb1tLVzIke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32af1eba-5be0-4288-e5f2-decadf42fa9c"
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)} # asignamos un número a cada vocablo\n",
        "idx2char = np.array(vocab)\n",
        "#-----------revisando las conversiones\n",
        "\n",
        "#for char,_ in zip(char2idx, range(len(vocab))):\n",
        "    #print(' {:4s}: {:3d},'.format(repr(char),char2idx[char]))\n",
        "\n",
        "#pasamos todo el texto a números\n",
        "texto_como_entero= np.array([char2idx[c] for c in texto])\n",
        "print('texto: {}'.format(repr(texto[:100])))\n",
        "print('{}'.format(repr(texto_como_entero[:100])))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texto: 'las escrituras de restauracion edicion del nombre verdadero con un contenido tanto de la tanaj y el '\n",
            "array([22, 11, 29,  0, 15, 29, 13, 28, 19, 30, 31, 28, 11, 29,  0, 14, 15,\n",
            "        0, 28, 15, 29, 30, 11, 31, 28, 11, 13, 19, 25, 24,  0, 15, 14, 19,\n",
            "       13, 19, 25, 24,  0, 14, 15, 22,  0, 24, 25, 23, 12, 28, 15,  0, 32,\n",
            "       15, 28, 14, 11, 14, 15, 28, 25,  0, 13, 25, 24,  0, 31, 24,  0, 13,\n",
            "       25, 24, 30, 15, 24, 19, 14, 25,  0, 30, 11, 24, 30, 25,  0, 14, 15,\n",
            "        0, 22, 11,  0, 30, 11, 24, 11, 20,  0, 35,  0, 15, 22,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl6mkWE6u2iH"
      },
      "source": [
        "rows=[]\n",
        "columns=['num','vocab']\n",
        "for i, voc in enumerate(vocab):\n",
        "  #print(i,'-->', voc)\n",
        "  rows.append([i,voc])\n",
        "df= pd.DataFrame(columns=['num','vocab'],data=rows)\n",
        "df.head(10)\n",
        "df.to_csv('data_vocab.csv',index=False)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky_cT7xN4OiO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7698516-de18-460a-d2c4-27bc915dafd2"
      },
      "source": [
        "char_dataset= tf.data.Dataset.from_tensor_slices(texto_como_entero)\n",
        "#cantidad de secuencia de caracteres\n",
        "secu_length=200\n",
        "#creamos secuencias de maximo 100 caractereres\n",
        "secuencias= char_dataset.batch(secu_length+1, drop_remainder=True)\n",
        "for item in secuencias.take(10):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'las escrituras de restauracion edicion del nombre verdadero con un contenido tanto de la tanaj y el brit renovado la primera edicion en espanol traducida de la tercera edicion actualizada en ingles sup'\n",
            "'ervision escritural y administracion doctrinal por el ramyk rabi moshe yosef koniuchowsky editorial sus brazos a yisrae yirmeyahu jeremias 31 31 37 31 los dias vienen dice en que hare brit chadasha bri'\n",
            "'t renovado con bayit yisrael y con bayit yahudah hine yamim baim neum yahweh vecharati et bayit yisrael veet bayit yahudah brit chadasha 32 no sera como el brit que hice con sus padres cuando los tome '\n",
            "'de la mano y los saque de la tierra de mitzrayim porque ellos por su parte violaron mi brit aunque yo fui un esposo para ellos dice lo chabrit asher karati et avotam beyom hecheziki veyadam lehotsiam m'\n",
            "'eerets mitzrayim asher hema heferu et brit veanochi baalti vam neum yahweh 33 porque este es el brit que hare con la bayit yisrael despues de esos dias dice yo pondre mi torah en sus mentes y la escrib'\n",
            "'ire en sus levavot yo sere su elohim y ellos seran ami mi pueblo ki zot habrit asher echrot et beyt yisrael acharey hayamim hahem neum yahweh natati et torati bekirbam veal libam echtavena vehayiti lah'\n",
            "'em lelohim vehema yihyu li leam 34 ninguno de ellos ensenara mas a su hermano diciendo conoce a porque todos me conoceran desde el menor de ellos hasta el mas grande dice porque yo perdonare sus transg'\n",
            "'resiones y nunca mas me acordare de sus pecados velo yelamdu od ish et reehu veish et achiv lemor deu et yahweh ki chulam yedu oti lemiktanam vead gedolam neumyahweh ki eslach laavonam ulechatatam lo e'\n",
            "'zkar od 35 asi dice quien da el sol para luz del dia la luna y las cochavim para la luz de la noche y hace al mar rugir tzevaot es su nombre ko amar yahweh noten shemesh leor yomam chukot yareach vecho'\n",
            "'chavim leor layla roga hayam vayehemu galav yahweh tzevaot shemo 36 si estos chukim desaparecen delante de mi presencia dice entonces la zera de yisrael dejara de ser una nacion en mi presencia todos l'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l6Tobzz7hww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b41da9cd-5a92-4610-a4e5-92300298a971"
      },
      "source": [
        "#funcion para obtener el conjunto de datos de trainning\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text= chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset  = secuencias.map(split_input_target)\n",
        "#el dataset contiene un conjunto de parejas de secuencia de texto\n",
        "#(con la representación numérica de los caracteres), donde el \n",
        "#primer componente de la pareja contiene un paquete con una secuencia \n",
        "#de 100 caracteres del texto original y la segunda su correspondiente salida, \n",
        "#también de 100 caracteres. )\n",
        "for input_example, target_example in dataset.take(1):\n",
        "  print('input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print('Target data: ', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input data:  'las escrituras de restauracion edicion del nombre verdadero con un contenido tanto de la tanaj y el brit renovado la primera edicion en espanol traducida de la tercera edicion actualizada en ingles su'\n",
            "Target data:  'as escrituras de restauracion edicion del nombre verdadero con un contenido tanto de la tanaj y el brit renovado la primera edicion en espanol traducida de la tercera edicion actualizada en ingles sup'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UM54Sfe9x80",
        "outputId": "b861ca1c-348a-418a-84b2-3022e33f3945"
      },
      "source": [
        "#imprimimos el tensor del dataset\n",
        "print(dataset)\n",
        "#Hyper-Parametros para entrenamiento  de una rede neuronal \n",
        "#   -los datos se agrupan en batch\n",
        "BATCH_SIZE= 64\n",
        "#    -Tamaño de memoria disponible \n",
        "BUFFER_SIZE=10000\n",
        "dataset= dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print (dataset)\n",
        "#En el tensor dataset disponemos los datos de entrenamiento\n",
        "#con agrupamienttos (batches) compuestos de 64 parejas de secuencias \n",
        "#de 100 integers de 64 bits que representan el carácter correspondiente \n",
        "#en el vocabulario."
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<MapDataset shapes: ((200,), (200,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((64, 200), (64, 200)), types: (tf.int64, tf.int64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox_5lKZh_qUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8863fe3f-eb31-4068-c05b-0bc93f2c8a2c"
      },
      "source": [
        "#como es un problema de clasificación estándar \n",
        "#para el que debemos definir la función de Lossy el optimizador.\n",
        "def lossy(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "def create_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  #creando el modelo\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                         return_sequences=True,\n",
        "                         stateful=True,\n",
        "                         recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)                               \n",
        "  ])\n",
        "  #En cuanto al optimizador usaremos tf.keras.optimizers.Adam \n",
        "  #con los argumentos por defecto del optimizador Adam. \n",
        "  model.compile(optimizer='adam',\n",
        "              loss=lossy,\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "vocab_size= len(vocab)\n",
        "#dimensiones de los vectores que tendrá la capa.\n",
        "embedding_dim= 256\n",
        "#cantidad de neuronas\n",
        "rnn_units=1024\n",
        "#creamos nuestra red neuronal RNN\n",
        "model=create_model(vocab_size   =vocab_size,\n",
        "                  embedding_dim =embedding_dim,\n",
        "                  rnn_units     =rnn_units,\n",
        "                  batch_size    =BATCH_SIZE)\n",
        "#summary()para visualizar la estructura del modelo\n",
        "model.summary()\n",
        "#resultados=  -La capa LSTM consta más de 5 millones de parametros)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (64, None, 256)           9472      \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (64, None, 37)            37925     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,294,373\n",
            "Trainable params: 5,294,373\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L24WhqeauMWk",
        "outputId": "b6ccd8d0-801b-4a1f-94b4-f782bf765cfd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbeXxiWLF5hN"
      },
      "source": [
        "checkpoint_dir='/content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/'\n",
        "checkpoint_prefix= os.path.join(checkpoint_dir,\"cp_{epoch:04d}.ckpt\")\n",
        "\n",
        "\n",
        "cp_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
        "                                               monitor='loss',\n",
        "                                               verbose=1,\n",
        "                                               save_weights_only=True,\n",
        "                                               save_best_only=True,\n",
        "                                               mode='auto')\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l5cnXgLQI7H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "017597f5-2a63-4e3a-8dc6-e8ddd041e089"
      },
      "source": [
        "EPOCHS=20\n",
        "history=model.fit(dataset, \n",
        "                  epochs=EPOCHS, \n",
        "                  verbose=1,\n",
        "                  callbacks=[cp_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "381/381 [==============================] - ETA: 0s - loss: 2.0424 - accuracy: 0.3771\n",
            "Epoch 00001: loss improved from inf to 2.04236, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0001.ckpt\n",
            "381/381 [==============================] - 136s 344ms/step - loss: 2.0424 - accuracy: 0.3771\n",
            "Epoch 2/20\n",
            "381/381 [==============================] - ETA: 0s - loss: 1.3884 - accuracy: 0.5698\n",
            "Epoch 00002: loss improved from 2.04236 to 1.38844, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0002.ckpt\n",
            "381/381 [==============================] - 132s 339ms/step - loss: 1.3884 - accuracy: 0.5698\n",
            "Epoch 3/20\n",
            "381/381 [==============================] - ETA: 0s - loss: 1.2270 - accuracy: 0.6166\n",
            "Epoch 00003: loss improved from 1.38844 to 1.22697, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0003.ckpt\n",
            "381/381 [==============================] - 132s 337ms/step - loss: 1.2270 - accuracy: 0.6166\n",
            "Epoch 4/20\n",
            "381/381 [==============================] - ETA: 0s - loss: 1.1561 - accuracy: 0.6370\n",
            "Epoch 00004: loss improved from 1.22697 to 1.15610, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0004.ckpt\n",
            "381/381 [==============================] - 132s 337ms/step - loss: 1.1561 - accuracy: 0.6370\n",
            "Epoch 5/20\n",
            "381/381 [==============================] - ETA: 0s - loss: 1.1098 - accuracy: 0.6507\n",
            "Epoch 00005: loss improved from 1.15610 to 1.10979, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0005.ckpt\n",
            "381/381 [==============================] - 132s 337ms/step - loss: 1.1098 - accuracy: 0.6507\n",
            "Epoch 6/20\n",
            "381/381 [==============================] - ETA: 0s - loss: 1.0749 - accuracy: 0.6609\n",
            "Epoch 00006: loss improved from 1.10979 to 1.07494, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0006.ckpt\n",
            "381/381 [==============================] - 133s 340ms/step - loss: 1.0749 - accuracy: 0.6609\n",
            "Epoch 7/20\n",
            "381/381 [==============================] - ETA: 0s - loss: 1.0455 - accuracy: 0.6698\n",
            "Epoch 00007: loss improved from 1.07494 to 1.04547, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0007.ckpt\n",
            "381/381 [==============================] - 134s 343ms/step - loss: 1.0455 - accuracy: 0.6698\n",
            "Epoch 8/20\n",
            "381/381 [==============================] - ETA: 0s - loss: 1.0197 - accuracy: 0.6778\n",
            "Epoch 00008: loss improved from 1.04547 to 1.01975, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0008.ckpt\n",
            "381/381 [==============================] - 133s 341ms/step - loss: 1.0197 - accuracy: 0.6778\n",
            "Epoch 9/20\n",
            "381/381 [==============================] - ETA: 0s - loss: 0.9961 - accuracy: 0.6851\n",
            "Epoch 00009: loss improved from 1.01975 to 0.99615, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0009.ckpt\n",
            "381/381 [==============================] - 133s 341ms/step - loss: 0.9961 - accuracy: 0.6851\n",
            "Epoch 10/20\n",
            "381/381 [==============================] - ETA: 0s - loss: 0.9739 - accuracy: 0.6920\n",
            "Epoch 00010: loss improved from 0.99615 to 0.97394, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0010.ckpt\n",
            "381/381 [==============================] - 133s 341ms/step - loss: 0.9739 - accuracy: 0.6920\n",
            "Epoch 11/20\n",
            "381/381 [==============================] - ETA: 0s - loss: 0.9531 - accuracy: 0.6989\n",
            "Epoch 00011: loss improved from 0.97394 to 0.95308, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0011.ckpt\n",
            "381/381 [==============================] - 134s 344ms/step - loss: 0.9531 - accuracy: 0.6989\n",
            "Epoch 12/20\n",
            "381/381 [==============================] - ETA: 0s - loss: 0.9325 - accuracy: 0.7055\n",
            "Epoch 00012: loss improved from 0.95308 to 0.93254, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0012.ckpt\n",
            "381/381 [==============================] - 135s 344ms/step - loss: 0.9325 - accuracy: 0.7055\n",
            "Epoch 13/20\n",
            "381/381 [==============================] - ETA: 0s - loss: 0.9131 - accuracy: 0.7121\n",
            "Epoch 00013: loss improved from 0.93254 to 0.91308, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0013.ckpt\n",
            "381/381 [==============================] - 135s 345ms/step - loss: 0.9131 - accuracy: 0.7121\n",
            "Epoch 14/20\n",
            "381/381 [==============================] - ETA: 0s - loss: 0.8950 - accuracy: 0.7180\n",
            "Epoch 00014: loss improved from 0.91308 to 0.89503, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0014.ckpt\n",
            "381/381 [==============================] - 134s 342ms/step - loss: 0.8950 - accuracy: 0.7180\n",
            "Epoch 15/20\n",
            "381/381 [==============================] - ETA: 0s - loss: 0.8770 - accuracy: 0.7240\n",
            "Epoch 00015: loss improved from 0.89503 to 0.87700, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0015.ckpt\n",
            "381/381 [==============================] - 133s 340ms/step - loss: 0.8770 - accuracy: 0.7240\n",
            "Epoch 16/20\n",
            "102/381 [=======>......................] - ETA: 1:36 - loss: 0.8383 - accuracy: 0.7377"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o4h9cq8ac68"
      },
      "source": [
        "#creamos un modelo con iguales caracteristicas al 1° modelo\n",
        "model=create_model(vocab_size   =vocab_size,\n",
        "                  embedding_dim =embedding_dim,\n",
        "                  rnn_units     =rnn_units,\n",
        "                  batch_size    =BATCH_SIZE)\n",
        "\n",
        "#buscamos el ultimo checkpoint de entrenamiento\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "print(latest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGZCz4W3lkWa"
      },
      "source": [
        "# cargamos los pesos al nuevo modelo (estos valores tienes una variación de un 10%)\n",
        "model.load_weights(latest)\n",
        "# continuamos el entrenamiento desde el checkpoint en que quedamos\n",
        "history2=model.fit(dataset, \n",
        "                    epochs=20, \n",
        "                    verbose=1,\n",
        "                    callbacks=[cp_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x20xRy-UwzZ"
      },
      "source": [
        "# You can change the directory name\n",
        "LOG_DIR = 'tb_logs'\n",
        "\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "import os\n",
        "if not os.path.exists(LOG_DIR):\n",
        "  os.makedirs(LOG_DIR)\n",
        "  \n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR))\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ1L-Vm1ZrTU"
      },
      "source": [
        "tbCallBack = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, \n",
        "                         histogram_freq=1,\n",
        "                         write_graph=True,\n",
        "                         write_grads=True,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         write_images=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcLIbeJjGmDF"
      },
      "source": [
        "\n",
        "#model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)\n",
        "history_TB=model.fit(dataset, \n",
        "                    epochs=20, \n",
        "                    verbose=1,\n",
        "                    callbacks=[tbCallBack])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vGr7GvUzwxg"
      },
      "source": [
        "#creamos un modelo tomando como base el ultimo checkpoint\n",
        "model = create_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvoSiWWp2_H-"
      },
      "source": [
        "#funcion para generar texto\n",
        "def generate_text(model, start_string):\n",
        "  #definimos cuantos tensores/cantidad de texto generaremos\n",
        "  num_generate=500\n",
        "  #convertimos el texto en números\n",
        "  input_eval=[char2idx[s] for s in start_string]\n",
        "  input_eval= tf.expand_dims (input_eval,0)\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 0.2  #(0.0 a  1) entre más alta la temperatura más creatividad al modelo, pero tambien más errores ortograficos.\n",
        "  model.reset_states() #bucle para generar caracteres, mediante predicciones\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "    input_eval= tf.expand_dims([predicted_id],0)\n",
        "    text_generated.append (idx2char[predicted_id])\n",
        "  \n",
        "  return (start_string+ ''.join(text_generated))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkLdbX724kBy",
        "outputId": "b7d8a329-2c72-4201-8c76-3f330ddc6670"
      },
      "source": [
        "print(generate_text(model, start_string=u\"el rey\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "el rey nuevamente se alegrocon el marques el gato qui una hermosa herencia dijo el rey al marques de carabas misericordia muy ufano con su presa fue al palacio del ogro quiso entrar el gato al oir el ruido del rey yuiente a sucarro y le dijo a su amo sigues mi cosechaban y les dijobuena gente que estan cosechando por el cobro todo el pobre patrimonioel mayor rel ogro habia mandadopreparar para sus amigos que vieramuerto aguardo a que algun conejillo poco cono con la princesa el gato se convirtio en gr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_BJSb-7pL7B",
        "outputId": "55eefe8a-8fcc-4a82-d879-a50acb35bb8b"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "import os\n",
        "dir_export= '/content/gdrive/MyDrive/Colab Notebooks/Modelos'\n",
        "#dir_export= os.path.join(dir_drive)\n",
        "# Serializamos el modelo en forma JSON\n",
        "model_json = model.to_json()\n",
        "with open(os.path.join(dir_export,'RNN_ElGatoConBotas_json.json'), 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(os.path.join(dir_export,'RNN_ElGatoConBotas_pesos.hdf5'))\n",
        "model.save(os.path.join(dir_export,'RNN_ElGatoConBotas_model.h5'))\n",
        "print(\"modelo salvado en Drive de google\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "modelo salvado en Drive de google\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R11dLzBzwv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a493ca0d-3802-4b0a-f974-fe080109fc6a"
      },
      "source": [
        "!wget https://github.com/GustavoAdolfoGuizaWalteros/Deep_Learning/blob/main/Modelos/RNN_ElGatoConBotas_pesos.hdf5?raw=true \\\n",
        "      -O RNN_ElGatoConBotas_model.h5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-24 16:36:27--  https://github.com/GustavoAdolfoGuizaWalteros/Deep_Learning/blob/main/Modelos/RNN_ElGatoConBotas_pesos.hdf5?raw=true\n",
            "Resolving github.com (github.com)... 52.192.72.89\n",
            "Connecting to github.com (github.com)|52.192.72.89|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/GustavoAdolfoGuizaWalteros/Deep_Learning/raw/main/Modelos/RNN_ElGatoConBotas_pesos.hdf5 [following]\n",
            "--2021-11-24 16:36:28--  https://github.com/GustavoAdolfoGuizaWalteros/Deep_Learning/raw/main/Modelos/RNN_ElGatoConBotas_pesos.hdf5\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/GustavoAdolfoGuizaWalteros/Deep_Learning/main/Modelos/RNN_ElGatoConBotas_pesos.hdf5 [following]\n",
            "--2021-11-24 16:36:28--  https://raw.githubusercontent.com/GustavoAdolfoGuizaWalteros/Deep_Learning/main/Modelos/RNN_ElGatoConBotas_pesos.hdf5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21134424 (20M) [application/octet-stream]\n",
            "Saving to: ‘RNN_ElGatoConBotas_model.h5’\n",
            "\n",
            "RNN_ElGatoConBotas_ 100%[===================>]  20.16M   117MB/s    in 0.2s    \n",
            "\n",
            "2021-11-24 16:36:30 (117 MB/s) - ‘RNN_ElGatoConBotas_model.h5’ saved [21134424/21134424]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w_jNNbCAul4",
        "outputId": "80ac9ee3-0b5f-410c-96c0-b2bec1f2db19"
      },
      "source": [
        "!pip install pyprind"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZcueKBiAqRd",
        "outputId": "b4fa1dfa-718b-4f00-b3ee-2c772b0f1c58"
      },
      "source": [
        "def reporthook(count, block_size, total_size):\n",
        "    global start_time\n",
        "    if count == 0:\n",
        "        start_time = time.time()\n",
        "        return\n",
        "    duration = time.time() - start_time\n",
        "    progress_size = int(count * block_size)\n",
        "    speed = progress_size / (1024.**2 * duration)\n",
        "    percent = count * block_size * 100. / total_size\n",
        "    sys.stdout.write(\"\\r%d%% | %d MB | %.2f MB/s | %d segundos transcurrido\" %\n",
        "                    (percent, progress_size / (1024.**2), speed, duration))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "import urllib.request\n",
        "url_github_Model='https://github.com/GustavoAdolfoGuizaWalteros/Deep_Learning/blob/main/Modelos/RNN_ElGatoConBotas_pesos.hdf5?raw=true'\n",
        "urllib.request.urlretrieve(url_github_Model,\n",
        "                           'RNN_ElGatoConBotas_model.h5', \n",
        "                           reporthook)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% | 20 MB | 3.50 MB/s | 5 segundos transcurrido"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('RNN_ElGatoConBotas_model.h5', <http.client.HTTPMessage at 0x7fd07d9d35d0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "Dewzz_pW9fFE",
        "outputId": "b52362c6-26e7-47f3-dd6d-74bf6dd34935"
      },
      "source": [
        "new_model = tf.keras.models.load_model('/content/RNN_ElGatoConBotas_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-3ea06602a136>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/RNN_ElGatoConBotas_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No model config found in the file at {filepath}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'decode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m       \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No model config found in the file at /content/RNN_ElGatoConBotas_model.h5."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFClGoVR2rFG"
      },
      "source": [
        "df2 = pd.read_csv(\"https://raw.githubusercontent.com/luisFernandoCastellanosG/Machine_learning/master/DeepLearning/PLN/recurrent_network_RNN/Modelos/data_vocab.csv\")\n",
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyHgx1AP2oz4"
      },
      "source": [
        "#funcion para generar texto\n",
        "def generate_text(model, start_string):\n",
        "  #definimos cuantos tensores/cantidad de texto generaremos\n",
        "  num_generate=500\n",
        "  #convertimos el texto en números\n",
        "  input_eval  = [char2idx[s] for s in start_string]\n",
        "  input_eval  = tf.expand_dims (input_eval,0)\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 0.2  #(0.0 a  1) entre más alta la temperatura más creatividad al modelo, pero tambien más errores ortograficos.\n",
        "  model.reset_states() #bucle para generar caracteres, mediante predicciones\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "    input_eval= tf.expand_dims([predicted_id],0)\n",
        "    text_generated.append (idx2char[predicted_id])\n",
        "  \n",
        "  return (start_string+ ''.join(text_generated))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-rwH5vd0oGc"
      },
      "source": [
        "print(generate_text(new_model, start_string=u\"los fantasmas de \"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}