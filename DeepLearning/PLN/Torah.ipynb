{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2_2_PLN_redes neuronales recurrentes (RNN).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "k6zNNLvrPR1f",
        "FshwyfFykB_4",
        "SKuJocQwCLg1"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GustavoAdolfoGuizaWalteros/Deep_Learning/blob/main/DeepLearning/PLN/Torah.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVozdGnPDqp3"
      },
      "source": [
        "#!kill -9 -1 #reiniciando la maquina virtual"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul9CHmeeRwy1",
        "outputId": "2dafcb52-5c37-46a5-bd81-73e83a8f7de3"
      },
      "source": [
        "#!pip list\n",
        "!nvcc --version  #Version de CUDA en la maquina virtual"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGa1tvf5t5r4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import timeit               #para medir tiempos\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import os\n",
        "import time\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXlRSrBJrK9T",
        "outputId": "503567ce-5275-4e64-f29b-03b183c83a9d"
      },
      "source": [
        "print(\"Tensorflow Version: \", tf.__version__)\n",
        "print(\"Dispositivos disponibles para entrenar: \", tf.config.list_physical_devices())\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Encontrada la GPU: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow Version:  2.7.0\n",
            "Dispositivos disponibles para entrenar:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Encontrada la GPU: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8hkb1wlQKET"
      },
      "source": [
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsqDzp_4Q0nV",
        "outputId": "52dc595c-8f8a-4bc4-f78e-f9db925eb919"
      },
      "source": [
        "cpu()  #ejecutamos entrenamiento con CPU\n",
        "gpu()  #ejecutamos entrenamiento con GPU\n",
        "# Run the op several times.\n",
        "print('TIEMPO (seg) para entrenar una red convolucional de 32x7x7x3 filtros sobre un randomico de 100x100x100x3 imagenes '\n",
        "      '(batch x height x width x channel). suma de 10 epochs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TIEMPO (seg) para entrenar una red convolucional de 32x7x7x3 filtros sobre un randomico de 100x100x100x3 imagenes (batch x height x width x channel). suma de 10 epochs.\n",
            "CPU (s):\n",
            "3.6845605359999922\n",
            "GPU (s):\n",
            "0.053426725000008446\n",
            "GPU speedup over CPU: 68x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9E2ihO2rLjM",
        "outputId": "6fb3ca3d-b7a9-462e-a9df-7596e8aac519"
      },
      "source": [
        "#tf.device('/gpu:0') #activando la CPU\n",
        "tf.device('/device:GPU:0') #activando la GPU "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.eager.context._EagerDeviceContext at 0x7f51c8465cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PETkDFl3Mff"
      },
      "source": [
        "fileDL= tf.keras.utils.get_file('Torah.txt','https://raw.githubusercontent.com/GustavoAdolfoGuizaWalteros/Deep_Learning/main/Cuentos_txt/Torah.txt')\n",
        "texto = open(fileDL, 'rb').read().decode(encoding='utf-8')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpxHpVkcys57",
        "outputId": "b3113157-0d99-4e2a-a8be-ece90a32c4f9"
      },
      "source": [
        "print('el texto tiene longitud de:{} caracteres'. format(len(texto)))\n",
        "vocab = sorted(set(texto))\n",
        "print('el texto esta compuesto de estos :{} caracteres'. format(len(vocab)))\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "el texto tiene longitud de:4906052 caracteres\n",
            "el texto esta compuesto de estos :37 caracteres\n",
            "[' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUZb1tLVzIke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32af1eba-5be0-4288-e5f2-decadf42fa9c"
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)} # asignamos un número a cada vocablo\n",
        "idx2char = np.array(vocab)\n",
        "#-----------revisando las conversiones\n",
        "\n",
        "#for char,_ in zip(char2idx, range(len(vocab))):\n",
        "    #print(' {:4s}: {:3d},'.format(repr(char),char2idx[char]))\n",
        "\n",
        "#pasamos todo el texto a números\n",
        "texto_como_entero= np.array([char2idx[c] for c in texto])\n",
        "print('texto: {}'.format(repr(texto[:100])))\n",
        "print('{}'.format(repr(texto_como_entero[:100])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texto: 'las escrituras de restauracion edicion del nombre verdadero con un contenido tanto de la tanaj y el '\n",
            "array([22, 11, 29,  0, 15, 29, 13, 28, 19, 30, 31, 28, 11, 29,  0, 14, 15,\n",
            "        0, 28, 15, 29, 30, 11, 31, 28, 11, 13, 19, 25, 24,  0, 15, 14, 19,\n",
            "       13, 19, 25, 24,  0, 14, 15, 22,  0, 24, 25, 23, 12, 28, 15,  0, 32,\n",
            "       15, 28, 14, 11, 14, 15, 28, 25,  0, 13, 25, 24,  0, 31, 24,  0, 13,\n",
            "       25, 24, 30, 15, 24, 19, 14, 25,  0, 30, 11, 24, 30, 25,  0, 14, 15,\n",
            "        0, 22, 11,  0, 30, 11, 24, 11, 20,  0, 35,  0, 15, 22,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl6mkWE6u2iH"
      },
      "source": [
        "rows=[]\n",
        "columns=['num','vocab']\n",
        "for i, voc in enumerate(vocab):\n",
        "  #print(i,'-->', voc)\n",
        "  rows.append([i,voc])\n",
        "df= pd.DataFrame(columns=['num','vocab'],data=rows)\n",
        "df.head(10)\n",
        "df.to_csv('Torah_data_vocab.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky_cT7xN4OiO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7698516-de18-460a-d2c4-27bc915dafd2"
      },
      "source": [
        "char_dataset= tf.data.Dataset.from_tensor_slices(texto_como_entero)\n",
        "#cantidad de secuencia de caracteres\n",
        "secu_length=300\n",
        "#creamos secuencias de maximo 100 caractereres\n",
        "secuencias= char_dataset.batch(secu_length+1, drop_remainder=True)\n",
        "for item in secuencias.take(10):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'las escrituras de restauracion edicion del nombre verdadero con un contenido tanto de la tanaj y el brit renovado la primera edicion en espanol traducida de la tercera edicion actualizada en ingles sup'\n",
            "'ervision escritural y administracion doctrinal por el ramyk rabi moshe yosef koniuchowsky editorial sus brazos a yisrae yirmeyahu jeremias 31 31 37 31 los dias vienen dice en que hare brit chadasha bri'\n",
            "'t renovado con bayit yisrael y con bayit yahudah hine yamim baim neum yahweh vecharati et bayit yisrael veet bayit yahudah brit chadasha 32 no sera como el brit que hice con sus padres cuando los tome '\n",
            "'de la mano y los saque de la tierra de mitzrayim porque ellos por su parte violaron mi brit aunque yo fui un esposo para ellos dice lo chabrit asher karati et avotam beyom hecheziki veyadam lehotsiam m'\n",
            "'eerets mitzrayim asher hema heferu et brit veanochi baalti vam neum yahweh 33 porque este es el brit que hare con la bayit yisrael despues de esos dias dice yo pondre mi torah en sus mentes y la escrib'\n",
            "'ire en sus levavot yo sere su elohim y ellos seran ami mi pueblo ki zot habrit asher echrot et beyt yisrael acharey hayamim hahem neum yahweh natati et torati bekirbam veal libam echtavena vehayiti lah'\n",
            "'em lelohim vehema yihyu li leam 34 ninguno de ellos ensenara mas a su hermano diciendo conoce a porque todos me conoceran desde el menor de ellos hasta el mas grande dice porque yo perdonare sus transg'\n",
            "'resiones y nunca mas me acordare de sus pecados velo yelamdu od ish et reehu veish et achiv lemor deu et yahweh ki chulam yedu oti lemiktanam vead gedolam neumyahweh ki eslach laavonam ulechatatam lo e'\n",
            "'zkar od 35 asi dice quien da el sol para luz del dia la luna y las cochavim para la luz de la noche y hace al mar rugir tzevaot es su nombre ko amar yahweh noten shemesh leor yomam chukot yareach vecho'\n",
            "'chavim leor layla roga hayam vayehemu galav yahweh tzevaot shemo 36 si estos chukim desaparecen delante de mi presencia dice entonces la zera de yisrael dejara de ser una nacion en mi presencia todos l'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l6Tobzz7hww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b41da9cd-5a92-4610-a4e5-92300298a971"
      },
      "source": [
        "#funcion para obtener el conjunto de datos de trainning\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text= chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset  = secuencias.map(split_input_target)\n",
        "#el dataset contiene un conjunto de parejas de secuencia de texto\n",
        "#(con la representación numérica de los caracteres), donde el \n",
        "#primer componente de la pareja contiene un paquete con una secuencia \n",
        "#de 100 caracteres del texto original y la segunda su correspondiente salida, \n",
        "#también de 100 caracteres. )\n",
        "for input_example, target_example in dataset.take(1):\n",
        "  print('input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print('Target data: ', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input data:  'las escrituras de restauracion edicion del nombre verdadero con un contenido tanto de la tanaj y el brit renovado la primera edicion en espanol traducida de la tercera edicion actualizada en ingles su'\n",
            "Target data:  'as escrituras de restauracion edicion del nombre verdadero con un contenido tanto de la tanaj y el brit renovado la primera edicion en espanol traducida de la tercera edicion actualizada en ingles sup'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UM54Sfe9x80",
        "outputId": "b861ca1c-348a-418a-84b2-3022e33f3945"
      },
      "source": [
        "#imprimimos el tensor del dataset\n",
        "print(dataset)\n",
        "#Hyper-Parametros para entrenamiento  de una rede neuronal \n",
        "#   -los datos se agrupan en batch\n",
        "BATCH_SIZE= 128\n",
        "#    -Tamaño de memoria disponible \n",
        "BUFFER_SIZE=10000\n",
        "dataset= dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print (dataset)\n",
        "#En el tensor dataset disponemos los datos de entrenamiento\n",
        "#con agrupamienttos (batches) compuestos de 64 parejas de secuencias \n",
        "#de 100 integers de 64 bits que representan el carácter correspondiente \n",
        "#en el vocabulario."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<MapDataset shapes: ((200,), (200,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((64, 200), (64, 200)), types: (tf.int64, tf.int64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox_5lKZh_qUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8863fe3f-eb31-4068-c05b-0bc93f2c8a2c"
      },
      "source": [
        "#como es un problema de clasificación estándar \n",
        "#para el que debemos definir la función de Lossy el optimizador.\n",
        "def lossy(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "def create_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  #creando el modelo\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                         return_sequences=True,\n",
        "                         stateful=True,\n",
        "                         recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)                               \n",
        "  ])\n",
        "  #En cuanto al optimizador usaremos tf.keras.optimizers.Adam \n",
        "  #con los argumentos por defecto del optimizador Adam. \n",
        "  model.compile(optimizer='adam',\n",
        "              loss=lossy,\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "vocab_size= len(vocab)\n",
        "#dimensiones de los vectores que tendrá la capa.\n",
        "embedding_dim= 256\n",
        "#cantidad de neuronas\n",
        "rnn_units=1024\n",
        "#creamos nuestra red neuronal RNN\n",
        "model=create_model(vocab_size   =vocab_size,\n",
        "                  embedding_dim =embedding_dim,\n",
        "                  rnn_units     =rnn_units,\n",
        "                  batch_size    =BATCH_SIZE)\n",
        "#summary()para visualizar la estructura del modelo\n",
        "model.summary()\n",
        "#resultados=  -La capa LSTM consta más de 5 millones de parametros)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (64, None, 256)           9472      \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (64, None, 37)            37925     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,294,373\n",
            "Trainable params: 5,294,373\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L24WhqeauMWk",
        "outputId": "b6ccd8d0-801b-4a1f-94b4-f782bf765cfd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbeXxiWLF5hN"
      },
      "source": [
        "checkpoint_dir='/content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/'\n",
        "checkpoint_prefix= os.path.join(checkpoint_dir,\"cp_{epoch:04d}.ckpt\")\n",
        "\n",
        "\n",
        "cp_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
        "                                               monitor='loss',\n",
        "                                               verbose=1,\n",
        "                                               save_weights_only=True,\n",
        "                                               save_best_only=True,\n",
        "                                               mode='auto')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l5cnXgLQI7H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358febab-1fcb-4728-c834-38e063a94144"
      },
      "source": [
        "EPOCHS=150\n",
        "history=model.fit(dataset, \n",
        "                  epochs=EPOCHS, \n",
        "                  verbose=1,\n",
        "                  callbacks=[cp_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 2.6892 - accuracy: 0.2065\n",
            "Epoch 00001: loss improved from inf to 2.68924, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0001.ckpt\n",
            "124/124 [==============================] - 133s 1s/step - loss: 2.6892 - accuracy: 0.2065\n",
            "Epoch 2/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 2.0312 - accuracy: 0.3596\n",
            "Epoch 00002: loss improved from 2.68924 to 2.03121, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0002.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 2.0312 - accuracy: 0.3596\n",
            "Epoch 3/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.6479 - accuracy: 0.4828\n",
            "Epoch 00003: loss improved from 2.03121 to 1.64791, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0003.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 1.6479 - accuracy: 0.4828\n",
            "Epoch 4/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.4294 - accuracy: 0.5525\n",
            "Epoch 00004: loss improved from 1.64791 to 1.42940, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0004.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 1.4294 - accuracy: 0.5525\n",
            "Epoch 5/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.3085 - accuracy: 0.5891\n",
            "Epoch 00005: loss improved from 1.42940 to 1.30846, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0005.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 1.3085 - accuracy: 0.5891\n",
            "Epoch 6/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.2346 - accuracy: 0.6108\n",
            "Epoch 00006: loss improved from 1.30846 to 1.23460, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0006.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 1.2346 - accuracy: 0.6108\n",
            "Epoch 7/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.1832 - accuracy: 0.6262\n",
            "Epoch 00007: loss improved from 1.23460 to 1.18318, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0007.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 1.1832 - accuracy: 0.6262\n",
            "Epoch 8/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.1442 - accuracy: 0.6376\n",
            "Epoch 00008: loss improved from 1.18318 to 1.14419, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0008.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 1.1442 - accuracy: 0.6376\n",
            "Epoch 9/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.1126 - accuracy: 0.6470\n",
            "Epoch 00009: loss improved from 1.14419 to 1.11257, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0009.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 1.1126 - accuracy: 0.6470\n",
            "Epoch 10/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.0853 - accuracy: 0.6552\n",
            "Epoch 00010: loss improved from 1.11257 to 1.08527, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0010.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 1.0853 - accuracy: 0.6552\n",
            "Epoch 11/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.0612 - accuracy: 0.6627\n",
            "Epoch 00011: loss improved from 1.08527 to 1.06115, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0011.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 1.0612 - accuracy: 0.6627\n",
            "Epoch 12/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.0391 - accuracy: 0.6695\n",
            "Epoch 00012: loss improved from 1.06115 to 1.03909, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0012.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 1.0391 - accuracy: 0.6695\n",
            "Epoch 13/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.0189 - accuracy: 0.6759\n",
            "Epoch 00013: loss improved from 1.03909 to 1.01887, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0013.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 1.0189 - accuracy: 0.6759\n",
            "Epoch 14/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.9998 - accuracy: 0.6821\n",
            "Epoch 00014: loss improved from 1.01887 to 0.99978, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0014.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.9998 - accuracy: 0.6821\n",
            "Epoch 15/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.9819 - accuracy: 0.6877\n",
            "Epoch 00015: loss improved from 0.99978 to 0.98188, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0015.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.9819 - accuracy: 0.6877\n",
            "Epoch 16/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.9642 - accuracy: 0.6935\n",
            "Epoch 00016: loss improved from 0.98188 to 0.96416, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0016.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.9642 - accuracy: 0.6935\n",
            "Epoch 17/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.9472 - accuracy: 0.6989\n",
            "Epoch 00017: loss improved from 0.96416 to 0.94718, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0017.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.9472 - accuracy: 0.6989\n",
            "Epoch 18/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.9311 - accuracy: 0.7041\n",
            "Epoch 00018: loss improved from 0.94718 to 0.93115, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0018.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.9311 - accuracy: 0.7041\n",
            "Epoch 19/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.9147 - accuracy: 0.7097\n",
            "Epoch 00019: loss improved from 0.93115 to 0.91472, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0019.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.9147 - accuracy: 0.7097\n",
            "Epoch 20/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.8991 - accuracy: 0.7148\n",
            "Epoch 00020: loss improved from 0.91472 to 0.89913, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0020.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.8991 - accuracy: 0.7148\n",
            "Epoch 21/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.8835 - accuracy: 0.7199\n",
            "Epoch 00021: loss improved from 0.89913 to 0.88354, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0021.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.8835 - accuracy: 0.7199\n",
            "Epoch 22/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.8689 - accuracy: 0.7249\n",
            "Epoch 00022: loss improved from 0.88354 to 0.86885, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0022.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.8689 - accuracy: 0.7249\n",
            "Epoch 23/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.8541 - accuracy: 0.7300\n",
            "Epoch 00023: loss improved from 0.86885 to 0.85406, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0023.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.8541 - accuracy: 0.7300\n",
            "Epoch 24/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.8400 - accuracy: 0.7347\n",
            "Epoch 00024: loss improved from 0.85406 to 0.83997, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0024.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.8400 - accuracy: 0.7347\n",
            "Epoch 25/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.8261 - accuracy: 0.7396\n",
            "Epoch 00025: loss improved from 0.83997 to 0.82609, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0025.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.8261 - accuracy: 0.7396\n",
            "Epoch 26/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.8118 - accuracy: 0.7445\n",
            "Epoch 00026: loss improved from 0.82609 to 0.81178, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0026.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.8118 - accuracy: 0.7445\n",
            "Epoch 27/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.7996 - accuracy: 0.7490\n",
            "Epoch 00027: loss improved from 0.81178 to 0.79963, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0027.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.7996 - accuracy: 0.7490\n",
            "Epoch 28/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.7863 - accuracy: 0.7534\n",
            "Epoch 00028: loss improved from 0.79963 to 0.78630, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0028.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.7863 - accuracy: 0.7534\n",
            "Epoch 29/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.7738 - accuracy: 0.7578\n",
            "Epoch 00029: loss improved from 0.78630 to 0.77379, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0029.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.7738 - accuracy: 0.7578\n",
            "Epoch 30/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.7622 - accuracy: 0.7618\n",
            "Epoch 00030: loss improved from 0.77379 to 0.76218, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0030.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.7622 - accuracy: 0.7618\n",
            "Epoch 31/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.7506 - accuracy: 0.7659\n",
            "Epoch 00031: loss improved from 0.76218 to 0.75061, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0031.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.7506 - accuracy: 0.7659\n",
            "Epoch 32/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.7398 - accuracy: 0.7699\n",
            "Epoch 00032: loss improved from 0.75061 to 0.73980, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0032.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.7398 - accuracy: 0.7699\n",
            "Epoch 33/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.7291 - accuracy: 0.7734\n",
            "Epoch 00033: loss improved from 0.73980 to 0.72909, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0033.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.7291 - accuracy: 0.7734\n",
            "Epoch 34/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.7187 - accuracy: 0.7771\n",
            "Epoch 00034: loss improved from 0.72909 to 0.71874, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0034.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.7187 - accuracy: 0.7771\n",
            "Epoch 35/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.7091 - accuracy: 0.7802\n",
            "Epoch 00035: loss improved from 0.71874 to 0.70906, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0035.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.7091 - accuracy: 0.7802\n",
            "Epoch 36/150\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.6997 - accuracy: 0.7839\n",
            "Epoch 00036: loss improved from 0.70906 to 0.69967, saving model to /content/gdrive/MyDrive/Colab Notebooks/P2_Deep_Learning/PLN/Proyecto/checkpoints/cp_0036.ckpt\n",
            "124/124 [==============================] - 130s 1s/step - loss: 0.6997 - accuracy: 0.7839\n",
            "Epoch 37/150\n",
            "  6/124 [>.............................] - ETA: 2:03 - loss: 0.6664 - accuracy: 0.7971"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o4h9cq8ac68"
      },
      "source": [
        "#creamos un modelo con iguales caracteristicas al 1° modelo\n",
        "model=create_model(vocab_size   =vocab_size,\n",
        "                  embedding_dim =embedding_dim,\n",
        "                  rnn_units     =rnn_units,\n",
        "                  batch_size    =BATCH_SIZE)\n",
        "\n",
        "#buscamos el ultimo checkpoint de entrenamiento\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "print(latest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGZCz4W3lkWa"
      },
      "source": [
        "# cargamos los pesos al nuevo modelo (estos valores tienes una variación de un 10%)\n",
        "model.load_weights(latest)\n",
        "# continuamos el entrenamiento desde el checkpoint en que quedamos\n",
        "history2=model.fit(dataset, \n",
        "                    epochs=150, \n",
        "                    verbose=1,\n",
        "                    callbacks=[cp_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x20xRy-UwzZ"
      },
      "source": [
        "# You can change the directory name\n",
        "LOG_DIR = 'tb_logs'\n",
        "\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "import os\n",
        "if not os.path.exists(LOG_DIR):\n",
        "  os.makedirs(LOG_DIR)\n",
        "  \n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR))\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ1L-Vm1ZrTU"
      },
      "source": [
        "tbCallBack = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, \n",
        "                         histogram_freq=1,\n",
        "                         write_graph=True,\n",
        "                         write_grads=True,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         write_images=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcLIbeJjGmDF"
      },
      "source": [
        "\n",
        "#model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)\n",
        "history_TB=model.fit(dataset, \n",
        "                    epochs=100, \n",
        "                    verbose=1,\n",
        "                    callbacks=[tbCallBack])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vGr7GvUzwxg"
      },
      "source": [
        "#creamos un modelo tomando como base el ultimo checkpoint\n",
        "model = create_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvoSiWWp2_H-"
      },
      "source": [
        "#funcion para generar texto\n",
        "def generate_text(model, start_string):\n",
        "  #definimos cuantos tensores/cantidad de texto generaremos\n",
        "  num_generate=500\n",
        "  #convertimos el texto en números\n",
        "  input_eval=[char2idx[s] for s in start_string]\n",
        "  input_eval= tf.expand_dims (input_eval,0)\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 0.2  #(0.0 a  1) entre más alta la temperatura más creatividad al modelo, pero tambien más errores ortograficos.\n",
        "  model.reset_states() #bucle para generar caracteres, mediante predicciones\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "    input_eval= tf.expand_dims([predicted_id],0)\n",
        "    text_generated.append (idx2char[predicted_id])\n",
        "  \n",
        "  return (start_string+ ''.join(text_generated))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkLdbX724kBy",
        "outputId": "b7d8a329-2c72-4201-8c76-3f330ddc6670"
      },
      "source": [
        "print(generate_text(model, start_string=u\"el rey\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "el rey nuevamente se alegrocon el marques el gato qui una hermosa herencia dijo el rey al marques de carabas misericordia muy ufano con su presa fue al palacio del ogro quiso entrar el gato al oir el ruido del rey yuiente a sucarro y le dijo a su amo sigues mi cosechaban y les dijobuena gente que estan cosechando por el cobro todo el pobre patrimonioel mayor rel ogro habia mandadopreparar para sus amigos que vieramuerto aguardo a que algun conejillo poco cono con la princesa el gato se convirtio en gr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_BJSb-7pL7B",
        "outputId": "55eefe8a-8fcc-4a82-d879-a50acb35bb8b"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "import os\n",
        "dir_export= '/content/gdrive/MyDrive/Colab Notebooks/Modelos'\n",
        "#dir_export= os.path.join(dir_drive)\n",
        "# Serializamos el modelo en forma JSON\n",
        "model_json = model.to_json()\n",
        "with open(os.path.join(dir_export,'RNN_Torah_json.json'), 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(os.path.join(dir_export,'RNN_Torah_pesos.hdf5'))\n",
        "model.save(os.path.join(dir_export,'RNN_Torah_model.h5'))\n",
        "print(\"modelo salvado en Drive de google\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "modelo salvado en Drive de google\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R11dLzBzwv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a493ca0d-3802-4b0a-f974-fe080109fc6a"
      },
      "source": [
        "!wget https://github.com/GustavoAdolfoGuizaWalteros/Deep_Learning/blob/main/Modelos/RNN_ElGatoConBotas_pesos.hdf5?raw=true \\\n",
        "      -O RNN_ElGatoConBotas_model.h5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-24 16:36:27--  https://github.com/GustavoAdolfoGuizaWalteros/Deep_Learning/blob/main/Modelos/RNN_ElGatoConBotas_pesos.hdf5?raw=true\n",
            "Resolving github.com (github.com)... 52.192.72.89\n",
            "Connecting to github.com (github.com)|52.192.72.89|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/GustavoAdolfoGuizaWalteros/Deep_Learning/raw/main/Modelos/RNN_ElGatoConBotas_pesos.hdf5 [following]\n",
            "--2021-11-24 16:36:28--  https://github.com/GustavoAdolfoGuizaWalteros/Deep_Learning/raw/main/Modelos/RNN_ElGatoConBotas_pesos.hdf5\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/GustavoAdolfoGuizaWalteros/Deep_Learning/main/Modelos/RNN_ElGatoConBotas_pesos.hdf5 [following]\n",
            "--2021-11-24 16:36:28--  https://raw.githubusercontent.com/GustavoAdolfoGuizaWalteros/Deep_Learning/main/Modelos/RNN_ElGatoConBotas_pesos.hdf5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21134424 (20M) [application/octet-stream]\n",
            "Saving to: ‘RNN_ElGatoConBotas_model.h5’\n",
            "\n",
            "RNN_ElGatoConBotas_ 100%[===================>]  20.16M   117MB/s    in 0.2s    \n",
            "\n",
            "2021-11-24 16:36:30 (117 MB/s) - ‘RNN_ElGatoConBotas_model.h5’ saved [21134424/21134424]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w_jNNbCAul4",
        "outputId": "80ac9ee3-0b5f-410c-96c0-b2bec1f2db19"
      },
      "source": [
        "!pip install pyprind"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZcueKBiAqRd",
        "outputId": "b4fa1dfa-718b-4f00-b3ee-2c772b0f1c58"
      },
      "source": [
        "def reporthook(count, block_size, total_size):\n",
        "    global start_time\n",
        "    if count == 0:\n",
        "        start_time = time.time()\n",
        "        return\n",
        "    duration = time.time() - start_time\n",
        "    progress_size = int(count * block_size)\n",
        "    speed = progress_size / (1024.**2 * duration)\n",
        "    percent = count * block_size * 100. / total_size\n",
        "    sys.stdout.write(\"\\r%d%% | %d MB | %.2f MB/s | %d segundos transcurrido\" %\n",
        "                    (percent, progress_size / (1024.**2), speed, duration))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "import urllib.request\n",
        "url_github_Model='https://github.com/GustavoAdolfoGuizaWalteros/Deep_Learning/blob/main/Modelos/RNN_ElGatoConBotas_pesos.hdf5?raw=true'\n",
        "urllib.request.urlretrieve(url_github_Model,\n",
        "                           'RNN_Torah_model.h5', \n",
        "                           reporthook)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% | 20 MB | 3.50 MB/s | 5 segundos transcurrido"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('RNN_ElGatoConBotas_model.h5', <http.client.HTTPMessage at 0x7fd07d9d35d0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dewzz_pW9fFE"
      },
      "source": [
        "new_model = tf.keras.models.load_model('/content/RNN_Torah_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFClGoVR2rFG"
      },
      "source": [
        "df2 = pd.read_csv(\"https://raw.githubusercontent.com/luisFernandoCastellanosG/Machine_learning/master/DeepLearning/PLN/recurrent_network_RNN/Modelos/data_vocab.csv\")\n",
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}